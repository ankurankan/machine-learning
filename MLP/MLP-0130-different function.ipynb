{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "import scipy.io\n",
    "import scipy\n",
    "import random\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    '''\n",
    "    For load data and change shape to (x,y). x is the traing images and y is the label of the x.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, file):\n",
    "\n",
    "        self.data = scipy.io.loadmat(file, squeeze_me=True, struct_as_record=False)['mnist']\n",
    "        self.train_images = self.data.train_images\n",
    "        self.test_images = self.data.test_images\n",
    "        self.train_labels = self.data.train_labels\n",
    "        self.test_labels = self.data.test_labels\n",
    "\n",
    "        # transform the raw data to binary data\n",
    "        self.train_images = self.transform_images(self.train_images)\n",
    "        self.test_images = self.transform_images(self.test_images)\n",
    "\n",
    "        # reshape the training images to [784,1]\n",
    "        self.train_images = [np.reshape(x, (784, 1)) for x in self.train_images]\n",
    "        self.test_images = [np.reshape(x, (784, 1)) for x in self.test_images]\n",
    "\n",
    "        labels = np.asarray(self.train_labels)\n",
    "        selector = (labels == 3) | (labels == 7)\n",
    "        self.train_labels = self.train_labels[selector]\n",
    "        self.train_images = np.array(self.train_images)\n",
    "        self.train_images = self.train_images[selector]\n",
    "        ls= []\n",
    "        for i in self.train_labels:\n",
    "            if i == 3:\n",
    "                ls.append([[1.],[0.]])\n",
    "            else:\n",
    "                ls.append([[0.],[1.]])\n",
    "        self.train_labels = ls\n",
    "        \n",
    "        self.training_data = tuple(zip(self.train_images, self.train_labels))\n",
    "        \n",
    "#       test part\n",
    "        labels = np.asarray(self.test_labels)\n",
    "        selector = (labels == 3) | (labels == 7)\n",
    "        self.test_labels = self.test_labels[selector]\n",
    "        self.test_images = np.array(self.test_images)\n",
    "        self.test_images = self.test_images[selector]\n",
    "        ls= []\n",
    "        for i in self.test_labels:\n",
    "            if i == 3:\n",
    "                ls.append([[1.],[0.]])\n",
    "            else:\n",
    "                ls.append([[0.],[1.]])\n",
    "        self.test_labels = ls\n",
    "        \n",
    "        self.test_data = tuple(zip(self.test_images, self.test_labels))\n",
    "        \n",
    "        \n",
    "    # function to reshape\n",
    "    def transform_images(self, data):\n",
    "\n",
    "        reshaped = data.reshape(data.shape[0] * data.shape[1], data.shape[2])\n",
    "        swapped_axes = np.swapaxes(reshaped, 0, 1)\n",
    "        return swapped_axes * (1.0/256)\n",
    "#         return (swapped_axes > 122) * 2.0 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = DataLoader('mnistALL.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        '''\n",
    "        sizes is the structure of the whole network shown by [first layer, second layer, ... , last year] as a list\n",
    "        different waies to initialize the weights and biases\n",
    "        '''\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "#better initialization of weights and biases \n",
    "#         self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "#         self.weights = [np.random.randn(y, x)/np.sqrt(x)\n",
    "#                         for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    # the feedward calculation of the network\n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "    # stochastic gradient descent\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        error = []\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in xrange(epochs):\n",
    "#             random.shuffle(training_data)\n",
    "# not shuffle yet\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                error.append(self.evaluate(test_data)/(len(test_data) * 1.0))\n",
    "                print \"Epoch {0}: {1} / {2}\".format(\n",
    "                    j, self.evaluate(test_data), n_test)\n",
    "                \n",
    "            else:\n",
    "                print \"Epoch {0} complete\".format(j)\n",
    "        return error\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "def to_percent(y, position):\n",
    "    # Ignore the passed in position. This has the effect of scaling the default\n",
    "    # tick locations.\n",
    "    s = str(100 * y)\n",
    "\n",
    "    # The percent symbol needs escaping in latex\n",
    "    if matplotlib.rcParams['text.usetex'] is True:\n",
    "        return s + r'$\\%$'\n",
    "    else:\n",
    "        return s + '%'\n",
    "formatter = FuncFormatter(to_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "Epoch 0: 1059 / 2038\n",
      "Epoch 1: 1328 / 2038\n",
      "Epoch 2: 1613 / 2038\n",
      "Epoch 3: 1744 / 2038\n",
      "Epoch 4: 1824 / 2038\n",
      "Epoch 5: 1858 / 2038\n",
      "Epoch 6: 1875 / 2038\n",
      "Epoch 7: 1891 / 2038\n",
      "Epoch 8: 1901 / 2038\n",
      "Epoch 9: 1908 / 2038\n",
      "Epoch 10: 1911 / 2038\n",
      "Epoch 11: 1918 / 2038\n",
      "Epoch 12: 1924 / 2038\n",
      "Epoch 13: 1927 / 2038\n",
      "Epoch 14: 1933 / 2038\n",
      "Epoch 15: 1936 / 2038\n",
      "Epoch 16: 1940 / 2038\n",
      "Epoch 17: 1940 / 2038\n",
      "Epoch 18: 1943 / 2038\n",
      "Epoch 19: 1944 / 2038\n",
      "Epoch 20: 1948 / 2038\n",
      "Epoch 21: 1951 / 2038\n",
      "Epoch 22: 1951 / 2038\n",
      "Epoch 23: 1952 / 2038\n",
      "Epoch 24: 1953 / 2038\n",
      "Epoch 25: 1953 / 2038\n",
      "Epoch 26: 1956 / 2038\n",
      "Epoch 27: 1957 / 2038\n",
      "Epoch 28: 1957 / 2038\n",
      "Epoch 29: 1957 / 2038\n",
      "Epoch 30: 1959 / 2038\n",
      "Epoch 31: 1959 / 2038\n",
      "Epoch 32: 1959 / 2038\n",
      "Epoch 33: 1959 / 2038\n",
      "Epoch 34: 1960 / 2038\n",
      "Epoch 35: 1961 / 2038\n",
      "Epoch 36: 1962 / 2038\n",
      "Epoch 37: 1966 / 2038\n",
      "Epoch 38: 1966 / 2038\n",
      "Epoch 39: 1966 / 2038\n",
      "Epoch 40: 1966 / 2038\n",
      "Epoch 41: 1968 / 2038\n",
      "Epoch 42: 1968 / 2038\n",
      "Epoch 43: 1968 / 2038\n",
      "Epoch 44: 1968 / 2038\n",
      "Epoch 45: 1971 / 2038\n",
      "Epoch 46: 1973 / 2038\n",
      "Epoch 47: 1972 / 2038\n",
      "Epoch 48: 1974 / 2038\n",
      "Epoch 49: 1975 / 2038\n",
      "Epoch 50: 1975 / 2038\n",
      "Epoch 51: 1975 / 2038\n",
      "Epoch 52: 1976 / 2038\n",
      "Epoch 53: 1976 / 2038\n",
      "Epoch 54: 1977 / 2038\n",
      "Epoch 55: 1977 / 2038\n",
      "Epoch 56: 1977 / 2038\n",
      "Epoch 57: 1977 / 2038\n",
      "Epoch 58: 1978 / 2038\n",
      "Epoch 59: 1978 / 2038\n",
      "Epoch 60: 1979 / 2038\n",
      "Epoch 61: 1979 / 2038\n",
      "Epoch 62: 1980 / 2038\n",
      "Epoch 63: 1981 / 2038\n",
      "Epoch 64: 1981 / 2038\n",
      "Epoch 65: 1982 / 2038\n",
      "Epoch 66: 1983 / 2038\n",
      "Epoch 67: 1984 / 2038\n",
      "Epoch 68: 1984 / 2038\n",
      "Epoch 69: 1984 / 2038\n",
      "Epoch 70: 1984 / 2038\n",
      "Epoch 71: 1984 / 2038\n",
      "Epoch 72: 1984 / 2038\n",
      "Epoch 73: 1985 / 2038\n",
      "Epoch 74: 1985 / 2038\n",
      "Epoch 75: 1985 / 2038\n",
      "Epoch 76: 1985 / 2038\n",
      "Epoch 77: 1985 / 2038\n",
      "Epoch 78: 1985 / 2038\n",
      "Epoch 79: 1985 / 2038\n",
      "Epoch 80: 1985 / 2038\n",
      "Epoch 81: 1985 / 2038\n",
      "Epoch 82: 1985 / 2038\n",
      "Epoch 83: 1985 / 2038\n",
      "Epoch 84: 1985 / 2038\n",
      "Epoch 85: 1985 / 2038\n",
      "Epoch 86: 1985 / 2038\n",
      "Epoch 87: 1986 / 2038\n",
      "Epoch 88: 1986 / 2038\n",
      "Epoch 89: 1986 / 2038\n",
      "Epoch 90: 1986 / 2038\n",
      "Epoch 91: 1986 / 2038\n",
      "Epoch 92: 1986 / 2038\n",
      "Epoch 93: 1986 / 2038\n",
      "Epoch 94: 1986 / 2038\n",
      "Epoch 95: 1986 / 2038\n",
      "Epoch 96: 1986 / 2038\n",
      "Epoch 97: 1986 / 2038\n",
      "Epoch 98: 1986 / 2038\n",
      "Epoch 99: 1986 / 2038\n",
      "Epoch 100: 1987 / 2038\n",
      "Epoch 101: 1987 / 2038\n",
      "Epoch 102: 1987 / 2038\n",
      "Epoch 103: 1987 / 2038\n",
      "Epoch 104: 1987 / 2038\n",
      "Epoch 105: 1987 / 2038\n",
      "Epoch 106: 1986 / 2038\n",
      "Epoch 107: 1986 / 2038\n",
      "Epoch 108: 1986 / 2038\n",
      "Epoch 109: 1986 / 2038\n",
      "Epoch 110: 1986 / 2038\n",
      "Epoch 111: 1986 / 2038\n",
      "Epoch 112: 1986 / 2038\n",
      "Epoch 113: 1987 / 2038\n",
      "Epoch 114: 1987 / 2038\n",
      "Epoch 115: 1987 / 2038\n",
      "Epoch 116: 1987 / 2038\n",
      "Epoch 117: 1988 / 2038\n",
      "Epoch 118: 1988 / 2038\n",
      "Epoch 119: 1988 / 2038\n",
      "Epoch 120: 1988 / 2038\n",
      "Epoch 121: 1989 / 2038\n",
      "Epoch 122: 1990 / 2038\n",
      "Epoch 123: 1990 / 2038\n",
      "Epoch 124: 1990 / 2038\n",
      "Epoch 125: 1990 / 2038\n",
      "Epoch 126: 1990 / 2038\n",
      "Epoch 127: 1990 / 2038\n",
      "Epoch 128: 1990 / 2038\n",
      "Epoch 129: 1990 / 2038\n",
      "Epoch 130: 1990 / 2038\n",
      "Epoch 131: 1991 / 2038\n",
      "Epoch 132: 1991 / 2038\n",
      "Epoch 133: 1991 / 2038\n",
      "Epoch 134: 1991 / 2038\n",
      "Epoch 135: 1991 / 2038\n",
      "Epoch 136: 1991 / 2038\n",
      "Epoch 137: 1991 / 2038\n",
      "Epoch 138: 1991 / 2038\n",
      "Epoch 139: 1991 / 2038\n",
      "Epoch 140: 1991 / 2038\n",
      "Epoch 141: 1991 / 2038\n",
      "Epoch 142: 1992 / 2038\n",
      "Epoch 143: 1992 / 2038\n",
      "Epoch 144: 1992 / 2038\n",
      "Epoch 145: 1992 / 2038\n",
      "Epoch 146: 1992 / 2038\n",
      "Epoch 147: 1992 / 2038\n",
      "Epoch 148: 1992 / 2038\n",
      "Epoch 149: 1992 / 2038\n"
     ]
    }
   ],
   "source": [
    "training_data = data.training_data\n",
    "validation_data = data.test_data\n",
    "\n",
    "print len(data.training_data[0][0])\n",
    "sigmoid_net = Network([784,100,2])\n",
    "sigmoid_error = sigmoid_net.SGD(training_data, 150, 10, 0.005, test_data=validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        '''\n",
    "        sizes is the structure of the whole network shown by [first layer, second layer, ... , last year] as a list\n",
    "        different waies to initialize the weights and biases\n",
    "        '''\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "# #better initialization of weights and biases \n",
    "#         self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "#         self.weights = [np.random.randn(y, x)/np.sqrt(x)\n",
    "#                         for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    # the feedward calculation of the network\n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = tanh(np.dot(w, a)+b)\n",
    "        return a\n",
    "    # stochastic gradient descent\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        error = []\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in xrange(epochs):\n",
    "#             random.shuffle(training_data)\n",
    "# not shuffle yet\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                error.append(self.evaluate(test_data)/(len(test_data) * 1.0))\n",
    "                print \"Epoch {0}: {1} / {2}\".format(\n",
    "                    j, self.evaluate(test_data), n_test)\n",
    "                \n",
    "            else:\n",
    "                print \"Epoch {0} complete\".format(j)\n",
    "        return error\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = tanh(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            tanh_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = tanh_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 1392 / 2038\n",
      "Epoch 1: 1486 / 2038\n",
      "Epoch 2: 1517 / 2038\n",
      "Epoch 3: 1544 / 2038\n",
      "Epoch 4: 1569 / 2038\n",
      "Epoch 5: 1585 / 2038\n",
      "Epoch 6: 1592 / 2038\n",
      "Epoch 7: 1604 / 2038\n",
      "Epoch 8: 1611 / 2038\n",
      "Epoch 9: 1619 / 2038\n",
      "Epoch 10: 1629 / 2038\n",
      "Epoch 11: 1630 / 2038\n",
      "Epoch 12: 1636 / 2038\n",
      "Epoch 13: 1643 / 2038\n",
      "Epoch 14: 1645 / 2038\n",
      "Epoch 15: 1648 / 2038\n",
      "Epoch 16: 1649 / 2038\n",
      "Epoch 17: 1649 / 2038\n",
      "Epoch 18: 1651 / 2038\n",
      "Epoch 19: 1652 / 2038\n",
      "Epoch 20: 1654 / 2038\n",
      "Epoch 21: 1656 / 2038\n",
      "Epoch 22: 1662 / 2038\n",
      "Epoch 23: 1665 / 2038\n",
      "Epoch 24: 1668 / 2038\n",
      "Epoch 25: 1670 / 2038\n",
      "Epoch 26: 1672 / 2038\n",
      "Epoch 27: 1674 / 2038\n",
      "Epoch 28: 1677 / 2038\n",
      "Epoch 29: 1676 / 2038\n",
      "Epoch 30: 1677 / 2038\n",
      "Epoch 31: 1677 / 2038\n",
      "Epoch 32: 1678 / 2038\n",
      "Epoch 33: 1680 / 2038\n",
      "Epoch 34: 1681 / 2038\n",
      "Epoch 35: 1681 / 2038\n",
      "Epoch 36: 1681 / 2038\n",
      "Epoch 37: 1683 / 2038\n",
      "Epoch 38: 1684 / 2038\n",
      "Epoch 39: 1683 / 2038\n",
      "Epoch 40: 1683 / 2038\n",
      "Epoch 41: 1682 / 2038\n",
      "Epoch 42: 1683 / 2038\n",
      "Epoch 43: 1685 / 2038\n",
      "Epoch 44: 1687 / 2038\n",
      "Epoch 45: 1687 / 2038\n",
      "Epoch 46: 1690 / 2038\n",
      "Epoch 47: 1691 / 2038\n",
      "Epoch 48: 1693 / 2038\n",
      "Epoch 49: 1693 / 2038\n",
      "Epoch 50: 1695 / 2038\n",
      "Epoch 51: 1698 / 2038\n",
      "Epoch 52: 1699 / 2038\n",
      "Epoch 53: 1698 / 2038\n",
      "Epoch 54: 1697 / 2038\n",
      "Epoch 55: 1697 / 2038\n",
      "Epoch 56: 1698 / 2038\n",
      "Epoch 57: 1698 / 2038\n",
      "Epoch 58: 1697 / 2038\n",
      "Epoch 59: 1698 / 2038\n",
      "Epoch 60: 1699 / 2038\n",
      "Epoch 61: 1700 / 2038\n",
      "Epoch 62: 1701 / 2038\n",
      "Epoch 63: 1702 / 2038\n",
      "Epoch 64: 1702 / 2038\n",
      "Epoch 65: 1702 / 2038\n",
      "Epoch 66: 1704 / 2038\n",
      "Epoch 67: 1704 / 2038\n",
      "Epoch 68: 1705 / 2038\n",
      "Epoch 69: 1705 / 2038\n",
      "Epoch 70: 1706 / 2038\n",
      "Epoch 71: 1706 / 2038\n",
      "Epoch 72: 1706 / 2038\n",
      "Epoch 73: 1707 / 2038\n",
      "Epoch 74: 1707 / 2038\n",
      "Epoch 75: 1708 / 2038\n",
      "Epoch 76: 1709 / 2038\n",
      "Epoch 77: 1708 / 2038\n",
      "Epoch 78: 1708 / 2038\n",
      "Epoch 79: 1706 / 2038\n",
      "Epoch 80: 1707 / 2038\n",
      "Epoch 81: 1707 / 2038\n",
      "Epoch 82: 1707 / 2038\n",
      "Epoch 83: 1707 / 2038\n",
      "Epoch 84: 1707 / 2038\n",
      "Epoch 85: 1708 / 2038\n",
      "Epoch 86: 1707 / 2038\n",
      "Epoch 87: 1707 / 2038\n",
      "Epoch 88: 1707 / 2038\n",
      "Epoch 89: 1707 / 2038\n",
      "Epoch 90: 1707 / 2038\n",
      "Epoch 91: 1706 / 2038\n",
      "Epoch 92: 1706 / 2038\n",
      "Epoch 93: 1705 / 2038\n",
      "Epoch 94: 1707 / 2038\n",
      "Epoch 95: 1707 / 2038\n",
      "Epoch 96: 1706 / 2038\n",
      "Epoch 97: 1705 / 2038\n",
      "Epoch 98: 1707 / 2038\n",
      "Epoch 99: 1707 / 2038\n",
      "Epoch 100: 1708 / 2038\n",
      "Epoch 101: 1708 / 2038\n",
      "Epoch 102: 1708 / 2038\n",
      "Epoch 103: 1708 / 2038\n",
      "Epoch 104: 1712 / 2038\n",
      "Epoch 105: 1712 / 2038\n",
      "Epoch 106: 1713 / 2038\n",
      "Epoch 107: 1713 / 2038\n",
      "Epoch 108: 1713 / 2038\n",
      "Epoch 109: 1713 / 2038\n",
      "Epoch 110: 1713 / 2038\n",
      "Epoch 111: 1712 / 2038\n",
      "Epoch 112: 1712 / 2038\n",
      "Epoch 113: 1711 / 2038\n",
      "Epoch 114: 1711 / 2038\n",
      "Epoch 115: 1711 / 2038\n",
      "Epoch 116: 1711 / 2038\n",
      "Epoch 117: 1711 / 2038\n",
      "Epoch 118: 1711 / 2038\n",
      "Epoch 119: 1711 / 2038\n",
      "Epoch 120: 1711 / 2038\n",
      "Epoch 121: 1711 / 2038\n",
      "Epoch 122: 1711 / 2038\n",
      "Epoch 123: 1709 / 2038\n",
      "Epoch 124: 1709 / 2038\n",
      "Epoch 125: 1711 / 2038\n",
      "Epoch 126: 1711 / 2038\n",
      "Epoch 127: 1712 / 2038\n",
      "Epoch 128: 1714 / 2038\n",
      "Epoch 129: 1714 / 2038\n",
      "Epoch 130: 1715 / 2038\n",
      "Epoch 131: 1715 / 2038\n",
      "Epoch 132: 1716 / 2038\n",
      "Epoch 133: 1717 / 2038\n",
      "Epoch 134: 1716 / 2038\n",
      "Epoch 135: 1717 / 2038\n",
      "Epoch 136: 1719 / 2038\n",
      "Epoch 137: 1721 / 2038\n",
      "Epoch 138: 1721 / 2038\n",
      "Epoch 139: 1720 / 2038\n",
      "Epoch 140: 1721 / 2038\n",
      "Epoch 141: 1722 / 2038\n",
      "Epoch 142: 1721 / 2038\n",
      "Epoch 143: 1721 / 2038\n",
      "Epoch 144: 1721 / 2038\n",
      "Epoch 145: 1721 / 2038\n",
      "Epoch 146: 1721 / 2038\n",
      "Epoch 147: 1722 / 2038\n",
      "Epoch 148: 1720 / 2038\n",
      "Epoch 149: 1722 / 2038\n"
     ]
    }
   ],
   "source": [
    "training_data = data.training_data\n",
    "validation_data = data.test_data\n",
    "tanh_net = Network([784,100,2])\n",
    "tanh_error = tanh_net.SGD(training_data, 150, 10, 0.005, test_data=validation_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEZCAYAAAC0HgObAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWa6cWMzMLFf/HzdzrtealE9mAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ba00f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoches = range(len(sigmoid_error))\n",
    "plt.plot(epoches, sigmoid_error,'b')\n",
    "plt.plot(epoches, tanh_error,'r')\n",
    "\n",
    "\n",
    "plt.title('Plot of sigmoid vs. tanh')\n",
    "plt.xlabel('epoches')# make axis labels\n",
    "plt.ylabel('accuracies')\n",
    "\n",
    "plt.xlim(0.0, 150)\n",
    "plt.ylim(0.4, 1.0)\n",
    "plt.gca().yaxis.set_major_formatter(formatter)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
