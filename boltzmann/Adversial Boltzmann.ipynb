{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import random\n",
    "import scipy\n",
    "import scipy.io\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_classifier(samples, labels):\n",
    "    training_samples = samples[labels == 1]\n",
    "    s_mean_clamped = np.squeeze(np.asarray(np.mean(training_samples, 0)))\n",
    "    s_cov_clamped = np.cov(training_samples.T)\n",
    "    m = s_mean_clamped\n",
    "    C = s_cov_clamped - np.dot(np.asmatrix(s_mean_clamped).T, np.asmatrix(s_mean_clamped))\n",
    "    delta = np.zeros(s_cov_clamped.shape)\n",
    "    np.fill_diagonal(delta, 1. / (1. - np.multiply(m, m)))\n",
    "    w = delta - np.linalg.inv(C)\n",
    "    theta = np.arctanh(m) - np.dot(w, m)\n",
    "    F = -0.5 * np.dot(np.dot(m, w), m) - np.dot(theta, m) + 0.5 * np.dot(1 + m, np.log(0.5 * (1 + m))) + 0.5 * np.dot(1 - m, np.log(0.5 * (1 - m)))\n",
    "    Z = np.exp(-F)\n",
    "    return w, theta, Z\n",
    "\n",
    "def calculate_probabilities(samples, w, theta, normalizing_constant):\n",
    "    return np.exp(-calculate_energy(samples, w, theta)) / normalizing_constant\n",
    "\n",
    "def calculate_energy(samples, w, theta):\n",
    "    f = np.dot(samples, np.dot(w, samples.T))\n",
    "    # Also allow samples consisting of one sample (an array, so f.ndim == 1)\n",
    "    # Therefore, only take the diagonal in the two dimensional case\n",
    "    if f.ndim == 2:\n",
    "        f = np.diagonal(f)\n",
    "    return np.squeeze(np.asarray(-0.5 * f - np.dot(theta.T, samples.T)))\n",
    "\n",
    "def generate_samples(s, w, theta, num_burn_in=50, num_samples=500, show_transition_probabilities=False):\n",
    "    num_neurons = w.shape[0]\n",
    "    \n",
    "    # Initialize the matrix of generated samples\n",
    "    X = np.empty((0, num_neurons))\n",
    "    \n",
    "    # Iterate (first generate some samples during the burn-in period and then gather the samples)\n",
    "    for iteration in range(num_samples):\n",
    "        for burn_in in range(num_burn_in + 1):\n",
    "            # Store the original value of s\n",
    "            s_original = s\n",
    "            # Calculate the flip probabilities\n",
    "            p_flip = 0.5 * (1 + np.tanh(np.multiply(-s, np.dot(w, s) + theta)))\n",
    "            # Calculate transition probabilities\n",
    "            p_transition = p_flip / float(num_neurons)\n",
    "            p_stay = 1 - np.sum(p_transition)\n",
    "            # Flip according to the probability distribution of flipping\n",
    "            if random.random() <= 1 - p_stay:\n",
    "                # Pick a random neuron\n",
    "                neuron = random.randint(1, num_neurons) - 1\n",
    "                if random.random() <= p_flip[neuron]:\n",
    "                    s[neuron] *= -1\n",
    "            # Add the state if the sample is not generated during the burn in period\n",
    "            if burn_in >= num_burn_in:\n",
    "                if show_transition_probabilities:\n",
    "                    print('Transition probabilities for ', s_original,':', p_transition, ' (stay probability: ', p_stay, ')')\n",
    "                X = np.vstack([X, s.T])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_samples = 70\n",
    "num_neurons = 50\n",
    "\n",
    "X_train = np.random.binomial(1, 0.5, (num_samples, num_neurons)) * 2 - 1\n",
    "y_train = (np.mean(X_train, axis=1) > 0) * 1\n",
    "\n",
    "X_test = np.random.binomial(1, 0.5, (num_samples, num_neurons)) * 2 - 1\n",
    "y_test = (np.mean(X_test, axis=1) > 0) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77142857142857146"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(alpha=1e-5, hidden_layer_sizes=(100, 10), random_state=1)\n",
    "clf.fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "sklearn.metrics.accuracy_score(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.772857142857 0.0118666055185\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for _ in range(10):\n",
    "    w_pos, theta_pos, Z_pos = train_classifier(X_train, y_train)\n",
    "    w_neg, theta_neg, Z_neg = train_classifier(X_train, 1 - y_train)\n",
    "\n",
    "    num_new_samples = 10\n",
    "    X = X_train\n",
    "    y = y_train\n",
    "    pos_samples = X_train[y_train == 1]\n",
    "    neg_samples = X_train[y_train == 0]\n",
    "    for _ in range(num_new_samples):\n",
    "        pos_sample = np.asmatrix(pos_samples[np.random.randint(0, pos_samples.shape[0]), :]).T\n",
    "        neg_sample = np.asmatrix(neg_samples[np.random.randint(0, neg_samples.shape[0]), :]).T\n",
    "        sample_pos = generate_samples(pos_sample, w_pos, theta_pos.T, 0, 1)\n",
    "        sample_neg = generate_samples(neg_sample, w_neg, theta_neg.T, 0, 1)\n",
    "        X = np.vstack([X, sample_pos])\n",
    "        y = np.hstack([y, 1])\n",
    "        X = np.vstack([X, sample_neg])\n",
    "        y = np.hstack([y, 0])\n",
    "\n",
    "    clf = MLPClassifier(alpha=1e-5, hidden_layer_sizes=(100, 10), random_state=1)\n",
    "    clf.fit(X, y)\n",
    "    predicted = clf.predict(X_test)\n",
    "    scores.append(sklearn.metrics.accuracy_score(y_test, predicted))\n",
    "    \n",
    "print(np.mean(scores), np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/anaconda3/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.682 0.0628967407741\n",
      "0.66 0.0464758001545\n"
     ]
    }
   ],
   "source": [
    "num_samples = 50\n",
    "num_neurons = 10\n",
    "\n",
    "scores0 = []\n",
    "scores1 = []\n",
    "for _ in range(10):\n",
    "    X_train = np.random.binomial(1, 0.5, (num_samples, num_neurons)) * 2 - 1\n",
    "    y_train = (np.mean(X_train, axis=1) >= 0) * 1\n",
    "\n",
    "    X_test = np.random.binomial(1, 0.5, (num_samples, num_neurons)) * 2 - 1\n",
    "    y_test = (np.mean(X_test, axis=1) >= 0) * 1\n",
    "\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    clf = MLPClassifier(alpha=1e-5, hidden_layer_sizes=(5, 5), random_state=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    predicted = clf.predict(X_test)\n",
    "    scores0.append(sklearn.metrics.accuracy_score(y_test, predicted))\n",
    "\n",
    "    noise = np.random.binomial(1, 0.2, X_train.shape) * -2 + 1\n",
    "    X_train_noise = np.multiply(X_train, noise)\n",
    "    w_pos, theta_pos, Z_pos = train_classifier(X_train_noise, y_train)\n",
    "    w_neg, theta_neg, Z_neg = train_classifier(X_train_noise, 1 - y_train)\n",
    "\n",
    "    num_new_samples = 10\n",
    "    X = X_train\n",
    "    y = y_train\n",
    "    pos_samples = X_train[y_train == 1]\n",
    "    neg_samples = X_train[y_train == 0]\n",
    "    for _ in range(num_new_samples):\n",
    "        pos_sample = np.asmatrix(pos_samples[np.random.randint(0, pos_samples.shape[0]), :]).T\n",
    "        neg_sample = np.asmatrix(neg_samples[np.random.randint(0, neg_samples.shape[0]), :]).T\n",
    "        sample_pos = generate_samples(pos_sample, w_pos, theta_pos.T, 500, 1)\n",
    "        sample_neg = generate_samples(neg_sample, w_neg, theta_neg.T, 500, 1)\n",
    "        X = np.vstack([X, sample_pos])\n",
    "        y = np.hstack([y, 1])\n",
    "        X = np.vstack([X, sample_neg])\n",
    "        y = np.hstack([y, 0])\n",
    "\n",
    "    clf = MLPClassifier(alpha=1e-5, hidden_layer_sizes=(5, 5), random_state=1)\n",
    "    clf.fit(X, y)\n",
    "    predicted = clf.predict(X_test)\n",
    "    scores1.append(sklearn.metrics.accuracy_score(y_test, predicted))\n",
    "    \n",
    "print(np.mean(scores0), np.std(scores0))\n",
    "print(np.mean(scores1), np.std(scores1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
